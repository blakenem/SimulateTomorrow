{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO54tp/6fUaRTfq40o+QcI/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# COVID-19 Epidemiology Modelling Tutorial\n","\n","---"],"metadata":{"id":"DurhbeDmk1b6"}},{"cell_type":"markdown","source":["## Overview\n","\n","This tutorial focuses on modelling the spread of disease during epidemics using Physics-Informed Neural Networks (PINNs). Modelling in epidemiology allows for oncoming spikes and dips in cases of a disease to be predicted. This information can be used to guide public health policy decisions.\n","\n","We will start by applying a PINN to predict the motion of a damped harmonic oscillator (a spring or pendulum with friction or air resistance being acknowledged), building on the procedure described [here](https://benmoseley.blog/my-research/so-what-is-a-physics-informed-neural-network/). Then, you will apply the same method to COVID-19 data from the US in 2020 to try to predict the spikes and dips in COVID cases that occurred."],"metadata":{"id":"V8lr3hiXlU1q"}},{"cell_type":"markdown","source":["To start, run the following cell to import the necessary libraries:"],"metadata":{"id":"Z4vSPY9Rs3ks"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1E-B92t_gu30"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","from IPython.display import clear_output"]},{"cell_type":"markdown","source":["You won't be doing any coding for the first half of this tutorial, but run the following cell to get the continuous feedback system set up:"],"metadata":{"id":"S0YspGpNE77k"}},{"cell_type":"code","source":["# @title\n","import torch\n","import torch.nn as nn\n","from IPython.core.magic import register_cell_magic\n","\n","# Reference implementation\n","class ReferenceCOVID_NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.fc1 = nn.Linear(1, 64)\n","        self.fc2 = nn.Linear(64, 64)\n","        self.fc3 = nn.Linear(64, 64)\n","        self.fc4 = nn.Linear(64, 64)\n","        self.fc5 = nn.Linear(64, 1)\n","\n","    def forward(self, x):\n","        x = torch.tanh(self.fc1(x))\n","        x = torch.tanh(self.fc2(x))\n","        x = torch.tanh(self.fc3(x))\n","        x = torch.tanh(self.fc4(x))\n","        x = self.fc5(x)\n","        return x\n","\n","# Function to perform checks\n","def perform_checks_covid_nn(student_model):\n","    torch.manual_seed(123)\n","    reference_model = ReferenceCOVID_NN()\n","\n","    # Check if the models have the same architecture\n","    ref_layers = list(reference_model.children())\n","    student_layers = list(student_model.children())\n","\n","    if len(ref_layers) != len(student_layers):\n","        print(f\"❌ Model has incorrect number of layers: Expected {len(ref_layers)}, Got {len(student_layers)}\")\n","        return\n","\n","    for i, (ref_layer, student_layer) in enumerate(zip(ref_layers, student_layers)):\n","        if not isinstance(ref_layer, type(student_layer)):\n","            print(f\"❌ Layer {i+1} is of incorrect type: Expected {type(ref_layer)}, Got {type(student_layer)}\")\n","            return\n","        if isinstance(ref_layer, nn.Linear):\n","            if ref_layer.in_features != student_layer.in_features or ref_layer.out_features != student_layer.out_features:\n","                print(f\"❌ Layer {i+1} has incorrect dimensions: Expected ({ref_layer.in_features}, {ref_layer.out_features}), Got ({student_layer.in_features}, {student_layer.out_features})\")\n","                return\n","\n","    # Check the forward pass with a random input\n","    test_input = torch.randn(1, 1)\n","    reference_output = reference_model(test_input)\n","    student_output = student_model(test_input)\n","\n","    if torch.allclose(reference_output, student_output, atol=1e-5):\n","        print(\"✅ This model is correct\")\n","    else:\n","        print(f\"❌ Model produces incorrect output: Expected {reference_output.detach().numpy()}, Got {student_output.detach().numpy()}\")\n","\n","# Define the cell magic function\n","@register_cell_magic\n","def check_covid_nn(line, cell):\n","    # Execute the student's code and retrieve the model\n","    exec(cell, globals())\n","\n","    # Check if 'COVID_NN' is defined in the global namespace\n","    if 'COVID_NN' not in globals():\n","        print(\"❌ COVID_NN class is not defined\")\n","        return\n","\n","    # Instantiate the student's model\n","    torch.manual_seed(123)\n","    student_model = COVID_NN()\n","\n","    # Perform the checks on the model\n","    perform_checks_covid_nn(student_model)\n","\n","# Function to check the model and optimiser\n","def check_model_and_optimiser():\n","    # Check if 'COVID_model' is defined and correct\n","    if 'COVID_model' not in globals():\n","        print(\"❌ COVID_model is not defined.\")\n","        return\n","    if not isinstance(COVID_model, COVID_NN):\n","        print(\"❌ COVID_model is not an instance of the expected model class.\")\n","        return\n","    print(\"✅ COVID_model is correctly initialized.\")\n","\n","    # Check if 'COVID_optimiser' is defined and correct\n","    if 'COVID_optimiser' not in globals():\n","        print(\"❌ COVID_optimiser is not defined.\")\n","        return\n","    if not isinstance(COVID_optimiser, torch.optim.Adam):\n","        print(\"❌ COVID_optimiser is not an instance of torch.optim.Adam.\")\n","        return\n","    if COVID_optimiser.param_groups[0]['lr'] != 1e-2:\n","        print(f\"❌ Learning rate is incorrect: Expected 1e-2, Got {COVID_optimiser.param_groups[0]['lr']}\")\n","        return\n","\n","    # Check that the optimiser is using the correct parameters\n","    reference_params = list(COVID_model.parameters())\n","    student_params = COVID_optimiser.param_groups[0]['params']\n","    if not all(p1.shape == p2.shape for p1, p2 in zip(reference_params, student_params)):\n","        print(\"❌ Optimiser parameters do not match the model parameters.\")\n","        return\n","\n","    print(\"✅ COVID_optimiser is correctly initialised with the correct learning rate and parameters.\")\n","\n","# Define the cell magic function\n","@register_cell_magic\n","def check_model_and_optimiser_cell(line, cell):\n","    # Execute the student's code\n","    exec(cell, globals())\n","    # Run checks\n","    check_model_and_optimiser()\n","\n","# Reference implementation for comparison\n","def reference_get_R(I):\n","    R = torch.zeros_like(I, requires_grad=True)\n","    end = np.floor(len(R)/2).astype(int)\n","    for count in range(1, end):\n","        ones = torch.zeros_like(I)\n","        ones[:len(R) - 2*count] = 1.0\n","        R = R + torch.roll(I * ones, 2*count)\n","    return R\n","\n","# Function to check the student's implementation\n","def check_get_R():\n","    # Check if 'get_R' is defined\n","    if 'get_R' not in globals():\n","        print(\"❌ The function 'get_R' is not defined.\")\n","        return\n","\n","    # Check if 'get_R' is a function\n","    if not callable(get_R):\n","        print(\"❌ 'get_R' is not callable.\")\n","        return\n","\n","    # Test the function with a sample input\n","    test_input = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0], requires_grad=True)\n","    try:\n","        student_output = get_R(test_input)\n","    except Exception as e:\n","        print(f\"❌ Error when running 'get_R': {e}\")\n","        return\n","\n","    # Check if the output is a tensor\n","    if not isinstance(student_output, torch.Tensor):\n","        print(\"❌ 'get_R' should return a tensor.\")\n","        return\n","\n","    # Check if the output requires gradients\n","    if not student_output.requires_grad:\n","        print(\"❌ Output tensor 'R' should require gradients.\")\n","        return\n","\n","    # Compare the student's output to the reference implementation\n","    reference_output = reference_get_R(test_input)\n","    if torch.allclose(student_output, reference_output, atol=1e-6):\n","        print(\"✅ 'get_R' produces the correct output.\")\n","    else:\n","        print(\"❌ 'get_R' does not produce the correct output.\")\n","\n","# Define the cell magic function\n","@register_cell_magic\n","def check_get_R_cell(line, cell):\n","    # Execute the student's code\n","    exec(cell, globals())\n","    # Run checks\n","    check_get_R()\n","\n","# Reference setup for comparison\n","torch.manual_seed(111)\n","reference_PINN_COVID_model = COVID_NN()\n","reference_optimiser = torch.optim.Adam(reference_PINN_COVID_model.parameters(), lr=1e-4)\n","reference_beta = torch.ones_like(t_physics, requires_grad=True)\n","reference_beta_optimiser = torch.optim.Adam([reference_beta], lr=5e-4)\n","\n","# Check if the optimiser is using the correct model parameters\n","def check_optimiser_params(optimiser, reference_optimiser):\n","    student_params = list(optimiser.param_groups[0]['params'])\n","    reference_params = list(reference_optimiser.param_groups[0]['params'])\n","\n","    if len(student_params) != len(reference_params):\n","        return False\n","\n","    for student_param, reference_param in zip(student_params, reference_params):\n","        if not torch.equal(student_param, reference_param):\n","            return False\n","    return True\n","\n","# Function to check the student's implementation\n","def check_PINN_COVID_setup():\n","    # Check if 'PINN_COVID_model' is defined\n","    if 'PINN_COVID_model' not in globals():\n","        print(\"❌ 'PINN_COVID_model' is not defined.\")\n","        return\n","\n","    # Check if 'PINN_COVID_model' is an instance of 'COVID_NN'\n","    if not isinstance(PINN_COVID_model, COVID_NN):\n","        print(\"❌ 'PINN_COVID_model' should be an instance of 'COVID_NN'.\")\n","        return\n","    else:\n","        print(\"✅ 'PINN_COVID_model' is correctly initialized as an instance of 'COVID_NN'.\")\n","\n","    # Check if 'optimiser' is defined\n","    if 'optimiser' not in globals():\n","        print(\"❌ 'optimiser' is not defined.\")\n","        return\n","\n","    # Check if 'optimiser' is an instance of 'torch.optim.Adam'\n","    if not isinstance(optimiser, torch.optim.Adam):\n","        print(\"❌ 'optimiser' should be an instance of 'torch.optim.Adam'.\")\n","        return\n","\n","    # Check the learning rate of the optimiser\n","    student_lr = optimiser.param_groups[0]['lr']\n","    if student_lr == 1e-4:\n","        print(\"✅ 'optimiser' learning rate is correct.\")\n","    else:\n","        print(f\"❌ 'optimiser' learning rate is incorrect: Expected 1e-4, Got {student_lr}\")\n","\n","    # Check if the optimiser is using the correct model parameters\n","    if check_optimiser_params(optimiser, reference_optimiser):\n","        print(\"✅ 'optimiser' is correctly linked to 'PINN_COVID_model' parameters.\")\n","    else:\n","        print(\"❌ 'optimiser' is not correctly linked to 'PINN_COVID_model' parameters.\")\n","\n","    # Check if 'beta' is defined\n","    if 'beta' not in globals():\n","        print(\"❌ 'beta' is not defined.\")\n","        return\n","\n","    # Check if 'beta_optimiser' is defined\n","    if 'beta_optimiser' not in globals():\n","        print(\"❌ 'beta_optimiser' is not defined.\")\n","        return\n","\n","    # Check if 'beta_optimiser' is an instance of 'torch.optim.Adam'\n","    if not isinstance(beta_optimiser, torch.optim.Adam):\n","        print(\"❌ 'beta_optimiser' should be an instance of 'torch.optim.Adam'.\")\n","        return\n","\n","    # Check the learning rate of the beta optimiser\n","    student_beta_lr = beta_optimiser.param_groups[0]['lr']\n","    if student_beta_lr == 5e-4:\n","        print(\"✅ 'beta_optimiser' learning rate is correct.\")\n","    else:\n","        print(f\"❌ 'beta_optimiser' learning rate is incorrect: Expected 5e-4, Got {student_beta_lr}\")\n","\n","    # Check if the beta optimiser is linked to 'beta'\n","    if check_optimiser_params(beta_optimiser, reference_beta_optimiser):\n","        print(\"✅ 'optimiser' is correctly linked to 'PINN_COVID_model' parameters.\")\n","    else:\n","        print(\"❌ 'optimiser' is not correctly linked to 'PINN_COVID_model' parameters.\")\n","\n","\n","# Define the cell magic function\n","@register_cell_magic\n","def check_PINN_COVID_setup_cell(line, cell):\n","    # Execute the student's code\n","    exec(cell, globals())\n","    # Run checks\n","    check_PINN_COVID_setup()"],"metadata":{"id":"BP-VoEeeFLMr","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In order to run a neural network, we need a dataset to train and make predictions on. We will use the analytic solution of the damped harmonic oscillator for this purpose, training our neural network on the initial motion of the oscillator then testing it by asking it to predict the trajectory after the training period.\n","\n","The following cell sets up the required data and plots the trajectory along with the points along the trajectory that the neural network will be trained on:"],"metadata":{"id":"yb0ypAdKtOXH"}},{"cell_type":"code","source":["def oscillator(d, w0, t):\n","    assert d < w0\n","    w = np.sqrt(w0**2-d**2)\n","    phi = np.arctan(-d/w)\n","    A = 1/(2*np.cos(phi))\n","    cos = torch.cos(phi+w*t)\n","    sin = torch.sin(phi+w*t)\n","    exp = torch.exp(-d*t)\n","    y  = exp*2*A*cos\n","    return y\n","\n","d, w0 = 2, 20\n","\n","# get the analytical solution over the full domain\n","t = torch.linspace(0,1,500).view(-1,1)\n","x = oscillator(d, w0, t).view(-1,1)\n","\n","# slice out a small number of points from the LHS of the domain\n","t_data = t[0:300:25]\n","x_data = x[0:300:25]\n","\n","plt.figure()\n","plt.plot(t, x, label=\"Exact solution\")\n","plt.xlabel('time t')\n","plt.ylabel('position x')\n","plt.scatter(t_data, x_data, color=\"tab:orange\", label=\"Training data\")\n","plt.legend()\n","plt.show()"],"metadata":{"id":"3-UX2a5JuSqG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Our aim is to train a neural network using the orange points, in hopes that it will output the entire blue curve as the trajectory after it has been trained. To do this, we will create a simple 'feed forward' neural network with one input (a time value) and one output (a position value) using PyTorch in the following cell:"],"metadata":{"id":"tW3vIplivCeq"}},{"cell_type":"code","source":["class ff_NN(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.fc1 = nn.Linear(1,32)\n","        self.fc2 = nn.Linear(32,32)\n","        self.fc3 = nn.Linear(32,32)\n","        self.fc4 = nn.Linear(32,32)\n","        self.fc5 = nn.Linear(32, 1)\n","\n","    def forward(self, x):\n","        x = torch.tanh(self.fc1(x))\n","        x = torch.tanh(self.fc2(x))\n","        x = torch.tanh(self.fc3(x))\n","        x = torch.tanh(self.fc4(x))\n","        x = self.fc5(x)\n","        return x"],"metadata":{"id":"DALHKzO8vTEM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, we will train the model on the first portion of the analytic trajectory using the mean squared error as the loss function and the Adam optimiser algorithm."],"metadata":{"id":"uTnDoEX4yQpA"}},{"cell_type":"code","source":["torch.manual_seed(123) # This is to ensure the same random numbers are used each run, so the results are reproducible\n","model = ff_NN()\n","optimiser = torch.optim.Adam(model.parameters(),lr=1e-3)\n","\n","for i in range(2500):\n","    optimiser.zero_grad()\n","    x_nn = model(t_data)\n","    loss = torch.mean((x_nn-x_data)**2)\n","    loss.backward()\n","    optimiser.step()\n","\n","    # plot the result as training progresses\n","    if (i+1) % 100 == 0 or i == 0:\n","        clear_output(wait=True)\n","        x_result = model(t).detach()\n","        plt.figure(figsize=(8,4))\n","        plt.plot(t,x, color=\"grey\", linewidth=2, alpha=0.8, label=\"Exact solution\")\n","        plt.plot(t,x_result, color=\"tab:blue\", linewidth=2, alpha=0.8, label=\"Neural network prediction\")\n","        plt.scatter(t_data, x_data, s=60, color=\"tab:orange\", alpha=0.4, label='Training data')\n","        l = plt.legend(loc=(1.01,0.34), frameon=False, fontsize=\"large\")\n","        plt.setp(l.get_texts(), color=\"k\")\n","        plt.xlim(-0.05, 1.05)\n","        plt.ylim(-1.1, 1.1)\n","        plt.text(1.065,0.7,\"Training step: %i\"%(i+1),fontsize=\"xx-large\",color=\"k\")\n","        plt.axis(\"off\")\n","        plt.show()"],"metadata":{"id":"yx9zrw4oyNxN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Clearly, the neural network correctly learned to interpolate the trajectory of the oscillator from the sampled points in the period on which it was trained. However, the neural network's prediction for the trajectory of the oscillator after this period is incorrect and physically nonsensical.\n"],"metadata":{"id":"i0xdZav7Ecto"}},{"cell_type":"markdown","source":["## Introducing PINNs\n","This is where PINNs come in. If we know some equations which physical reasoning dictates the system should obey, we can include the extent to which these equations are violated in the loss function used during training. Then, the neural network's output should not only be able to interpolate the trajectory in the period on which it was trained, but it should be able to make a physically-motivated guess about the subsequent motion by combining interpolation of training datapoints with the 'physical intuition' which we have granted it.\n","\n","For a damped harmonic oscillator, the equation we can impose of this type is a second order differential equation which is simply $F=ma$ for this system:\n","\n","\\begin{equation}\n","m\\frac{d^2x}{{dt}^2} +\\mu \\frac{dx}{dt}+kx=0\n","\\end{equation}\n","\n","$\\mu$ and $k$ are constants that define the motion of the system, but we can learn their values as well as the correct weights of the neural network during training on the initial motion of the oscillator:"],"metadata":{"id":"YZTeBhpJ1KEJ"}},{"cell_type":"code","source":["# Assemble a set of points across the entire period of time that we want to solve the trajectory for\n","t_physics = torch.linspace(0,1,30).view(-1,1).requires_grad_(True)\n","\n","# Initialise the model and parameters\n","torch.manual_seed(123) # This is to ensure the same random numbers are used each run, so the results are reproducible\n","PINN_model = ff_NN()\n","mu = torch.tensor([5.0], requires_grad=True)\n","k = torch.tensor([360.0], requires_grad=True)\n","PINN_optimiser = torch.optim.Adam(PINN_model.parameters(),lr=5e-4)\n","params_optimiser = torch.optim.Adam([mu,k],lr=3e-3)\n","\n","# Run training loop\n","for i in range(30000):\n","    PINN_optimiser.zero_grad()\n","    params_optimiser.zero_grad()\n","\n","    # compute the \"data loss\"\n","    x_nn = PINN_model(t_data)\n","    loss1 = torch.mean((x_nn-x_data)**2)# use mean squared error\n","\n","    # compute the \"physics loss\"\n","    x_nn_physics = PINN_model(t_physics)\n","    dx  = torch.autograd.grad(x_nn_physics, t_physics, torch.ones_like(x_nn_physics), create_graph=True)[0]# computes dy/dx\n","    dx2 = torch.autograd.grad(dx,  t_physics, torch.ones_like(dx),  create_graph=True)[0]# computes d^2y/dx^2\n","    physics = dx2 + mu*dx + k*x_nn_physics # computes the residual of the 1D harmonic oscillator differential equation\n","    loss2 = (1e-4)*torch.mean(physics**2)\n","\n","    # backpropagate joint loss\n","    loss = loss1 + loss2# add two loss terms together\n","    loss.backward()\n","    PINN_optimiser.step()\n","    # only start optimising mu and k after the interpolation has begun\n","    if i > 5000:\n","        params_optimiser.step()\n","\n","\n","    # plot the result as training progresses\n","    if (i+1) % 500 == 0 or i == 0:\n","        clear_output(wait=True)\n","        x_results = PINN_model(t).detach()\n","        plt.figure(figsize=(8,4))\n","        plt.plot(t.detach().numpy(),x, color=\"grey\", linewidth=2, alpha=0.8, label=\"Exact solution\")\n","        plt.plot(t.detach().numpy(),x_results, color=\"tab:blue\", linewidth=2, alpha=0.8, label=\"Neural network prediction\")\n","        plt.scatter(t_data.detach().numpy(), x_data, s=60, color=\"tab:orange\", alpha=0.4, label='Training data')\n","        l = plt.legend(loc=(1.01,0.34), frameon=False, fontsize=\"large\")\n","        plt.setp(l.get_texts(), color=\"k\")\n","        plt.xlim(-0.05, 1.05)\n","        plt.ylim(-1.1, 1.1)\n","        plt.text(1.065,0.7,\"Training step: %i\"%(i+1),fontsize=\"xx-large\",color=\"k\")\n","        plt.axis(\"off\")\n","        plt.show()\n","        print(f'mu={mu.item()}, k={k.item()}')"],"metadata":{"id":"F5RsvpIrj4Fs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Clearly, the PINN framework is much better at predicting the trajectory of the oscillator. Further, the correct values of the parameters in the model were successfully learned to within about a 1% error!\n","\n","All of this is promising - we want to make predictions about the dynamics of an epidemic and we have just seen that PINNs can combine information from past data with models for how we expect the system to react to get good predictions.\n","\n","## Predicting COVID-19 Trends"],"metadata":{"id":"9fG3m6-8DCgd"}},{"cell_type":"markdown","source":["Before we jump in applying the PINN framework to the COVID-19 prediction problem, let's first see how well a simple neural network trained to fit the data does in predicting trends.\n","\n","The following cell imports US national statistics on the COVID-19 pandemic from a [New York Times dataset](https://github.com/nytimes/covid-19-data/blob/master/us.csv):"],"metadata":{"id":"se6ML8MJERML"}},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","url = 'https://raw.githubusercontent.com/nytimes/covid-19-data/master/us.csv'\n","# Load the CSV file into a DataFrame\n","df = pd.read_csv(url)\n","\n","# Calculate the 'Currently Infected' count\n","df['Currently Infected'] = df['cases'] - df['deaths']\n","# Calculate the rolling difference over the previous ten days\n","df['Infected'] = df['Currently Infected'].diff(periods=10)\n","# Calculate recovered count:\n","df['Recovered'] = df['Currently Infected'].diff(periods=10)\n","\n","x = np.array(df['Infected'][60:300]).astype(np.float32)/1e6\n","t = np.arange(len(df))[60:300]\n","t = (t/239).astype(np.float32)\n","\n","x = torch.from_numpy(x).view(-1,1)\n","t = torch.from_numpy(t).view(-1,1)\n","\n","# slice out a small number of points from the LHS of the domain\n","t_data = t[0:110:5]\n","x_data = x[0:110:5]\n","\n","plt.figure()\n","plt.plot(t, x, label=\"Exact solution\")\n","plt.scatter(t_data, x_data, color=\"tab:orange\", label=\"Training data\")\n","plt.ylabel('Number of people with COVID (Millions)')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"zywLZgNgFYpq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Again, our aim will be to train a simple feed-forward neural network with tanh as the activation function on the orange datapoints. Then, we will ask the neural network for a prediction across the entire range of time plotted above.\n","\n","Set up a new neural network class like the ```ff_NN()``` class defined above, but with 64 nodes per layer."],"metadata":{"id":"bTHGSgVgFgrn"}},{"cell_type":"code","source":["%%check_covid_nn # Leave this here - it will give you feedback on your code when you run the cell\n","\n","class COVID_NN(nn.Module):\n","    # Put your code here:"],"metadata":{"id":"Gn8fbRzqp-x0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, initialise the model, define the optimiser and write the code for the training loop. Use the Adam optimiser with learning rate of 0.01."],"metadata":{"id":"D8uyCMZ3p9eo"}},{"cell_type":"code","source":["%%check_model_and_optimiser_cell # Leave this here - it will give you feedback on your code when you run the cell\n","\n","torch.manual_seed(123) # This is to ensure the same random numbers are used each run, so the results are reproducible\n","COVID_model =  # Initialise the model\n","COVID_optimiser =  # Define the optimiser"],"metadata":{"id":"F6d6Noe3OuW-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training loop\n","for i in range(5000):\n","    # Zero the gradient stored in the optimiser\n","    x_nn = # Call the neural network on t_data\n","    loss =  # Use the mean squared error as the loss\n","    # Get gradient of loss\n","    # Take a step with the optimiser\n","\n","    # plot the result as training progresses\n","    if (i+1) % 1000 == 0 or i == 0:\n","        clear_output(wait=True)\n","        x_results = COVID_model(t).detach()\n","        plt.figure(figsize=(6,4))\n","        plt.plot(t.detach().numpy(),x, color=\"grey\", linewidth=2, alpha=0.8, label=\"Exact solution\")\n","        plt.plot(t.detach().numpy(),x_results, color=\"tab:blue\", linewidth=2, alpha=0.8, label=\"Neural network prediction\")\n","        plt.scatter(t_data.detach().numpy(), x_data, s=60, color=\"tab:orange\", alpha=0.4, label='Training data')\n","        l = plt.legend(loc=(1.01,0.34), frameon=False, fontsize=\"large\")\n","        plt.setp(l.get_texts(), color=\"k\")\n","        plt.xlim(0.2, 1.3)\n","        plt.ylim(0, 1.4)\n","        plt.text(1.3,1.0,\"Training step: %i\"%(i+1),fontsize=\"xx-large\",color=\"k\")\n","        plt.axis(\"off\")\n","        plt.show()"],"metadata":{"id":"7pp1yNhTG4k5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Just like in the case of the oscillator, this naive approch is successful in interpolating the behaviour between the training datapoints, but is not of any real value for making predictions about the future.\n","\n","To apply a PINN, we need an analogue of the $F=ma$ differential equation for this system. There are plenty of models for epidemiology that can supply us with differential equations to use for this purpose - for this reason, several academic papers applying different epidemiological models in this way have been written. We will use the simplest model, which is known as the SIR model.\n","\n","SIR stands for Susceptible, Infected, Removed - those are the three classes that we split the population up into in this model. The susceptible group is the set of all people who haven't had the virus yet, the infected group is the set of people who are currently infected by the virus so are transmitters of it, and the removed group is the set of all people who can no longer be infected or transmit the virus (i.e. the set of people who recovered from or died due to the virus). We define the number of people in each group as $S(t)$, $I(t)$, and $R(t)$. Assuming a very large population (a valid assumption for our dataset), the population is approximately constant, so $S(t)+I(t)+R(t)=N$.\n","\n","Taking $\\gamma$ as the rate of recovery or death - about 0.1 days$^{-1}$ in the case of COVID-19, and $\\beta$ as the transmission rate, the movement of people from one group to another can be described by the following differential equations:\n","\n","$$\\frac{dS}{dt}=-\\frac{\\beta S I}{N}$$\n","$$\\frac{dI}{dt}=\\frac{\\beta S I}{N} - \\gamma I$$\n","$$\\frac{dR}{dt}= \\gamma I$$\n","\n","This simply says that rate of change changes in the number of infected people is proportional to the number of infected people times the fraction of the population that is susceptible minus the rate of recovery. We are only learning and predicting $I(t)$, so we would like to capture this information in a differential equation in terms of $I(t)$ only. We can do this by taking the first and last of these differential equations as definitions of the system.\n","\n","Using $S(t)+I(t)+R(t)=N$ and $\\frac{dR}{dt}= \\gamma I$, we get\n","$$S(t) = N- I(t)-\\gamma\\int\\limits_{0}^{t}I(t')dt'$$\n","Subbing this into the second differential equation, we get a condition for the system to obey if it satisfies the SIR model:\n","$$ \\frac{\\beta I(t)}{N}\\left(N-I(t)-\\gamma\\int\\limits_{0}^{t}I(t')dt'\\right) - \\gamma I(t) - \\frac{dI}{dt}=0$$\n","\n","Using the PINN framework, the left hand side of this expression will be included in our loss function.\n","\n","The integral for $R$ can be done using our data by simply adding together the entries in the array of $I$ values that came in increments of ten steps before. This means that for the first ten days, $R(t)=0$, on day eleven $R=I(\\text{day 1})$, and in general on day $n$ where $n>10$,\n","$$R(t)=\\sum\\limits_{i=1}^{\\text{floor}(n/10)}I(\\text{day }n-10i)$$\n","Write a definition for a function that does this below, taking every fifth day's $I(t)$ value as an input and outputting a PyTorch tensor that contains the value of $R(t)$ on the same days. The process must be capable of undergoing backpropogation for gradient descent when optimising the neural network, so you can't add slices to tensors, but you can get around this by using the product of a tensor of ones and zeros with the tensor that you want to take a slice of. You may also find the ```torch.roll()``` function useful for this function.\n","\n","(This is quite a tricky one, so in case you get stuck the answer is in the hidden code in the cell that follows.)"],"metadata":{"id":"LrZ08nXwJXRM"}},{"cell_type":"code","source":["%%check_get_R_cell # Leave this here - it will give you feedback on your code when you run the cell\n","def get_R(I):\n","    R = torch.zeros_like(I,requires_grad=True)\n","    # Start coding here:\n","\n","    return R"],"metadata":{"id":"2Oyck-0PnF7m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title\n","# My solution is:\n","\n","#def get_R(I):\n","#    R = torch.zeros_like(I,requires_grad=True)\n","#    end = np.floor(len(R)/2).astype(int)\n","#    for count in range(1,end):\n","#        ones = torch.zeros_like(I)\n","#        ones[:len(R)-2*count] = 1.0\n","#        R = R + torch.roll(I*ones, 2*count)\n","#    return R"],"metadata":{"cellView":"form","id":"5uWRpY4qnYdM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, we will use the function you just defined and the PINN framework to make predictions on the dynamics of COVID-19. Fill in the required fields to set up the training process and run the training loop. Use the Adam optimiser for both sets of parameters, with learning rate of 0.0001 for the neural network and 0.0005 for the values of beta at each point of time."],"metadata":{"id":"tiWsr2ZVnKma"}},{"cell_type":"code","source":["%%check_PINN_COVID_setup_cell # Leave this here - it will give you feedback on your code when you run the cell\n","\n","t_physics = t[::5].requires_grad_(True) # These are the points in time where we will evaluate the physics loss\n","g = 23.9 # This is the value of gamma that you get by rescaling 1/(10 days) with 239 days = 1 unit\n","\n","torch.manual_seed(111) # This is to ensure the same random numbers are used each run, so the results are reproducible\n","PINN_COVID_model =\n","optimiser =\n","beta = torch.ones_like(t_physics, requires_grad=True)\n","beta_optimiser ="],"metadata":{"id":"Jy8iNx-fP3FS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training loop:\n","for i in range(9500):\n","    # Zero the gradient stored in the optimiser\n","\n","    # compute the \"data loss\"\n","    x_nn = # Pass t_data to the model\n","    loss1 = # Use mean squared error\n","\n","    # compute the \"physics loss\"\n","    x_nn_physics = # Pass t_physics to the model\n","    dx = # Compute dy/dx\n","    physics = # Compute the residual of the equation\n","    loss2 = (1e-4)*torch.mean(physics**2)\n","\n","    # backpropagate the joint loss - no need to wait for a certain number of steps before starting to optimise beta\n","    loss = # add the two loss terms together\n","    # Get gradient of loss\n","    # Take a step with the neural network optimiser\n","    # Take a step with the beta optimiser\n","\n","    # plot the result as training progresses\n","    if (i+1) % 500 == 0:\n","        clear_output(wait=True)\n","        x_results = PINN_COVID_model(t).detach()\n","        plt.figure(figsize=(6,4))\n","        plt.plot(t.detach().numpy(),x, color=\"grey\", linewidth=2, alpha=0.8, label=\"Exact solution\")\n","        plt.plot(t.detach().numpy(),x_results, color=\"tab:blue\", linewidth=2, alpha=0.8, label=\"Neural network prediction\")\n","        plt.scatter(t_data.detach().numpy(), x_data, s=60, color=\"tab:orange\", alpha=0.4, label='Training data')\n","        l = plt.legend(loc=(1.01,0.34), frameon=False, fontsize=\"large\")\n","        plt.setp(l.get_texts(), color=\"k\")\n","        plt.xlim(0.2, 1.3)\n","        plt.ylim(0, 1.4)\n","        plt.text(1.3,1,\"Training step: %i\"%(i+1),fontsize=\"xx-large\",color=\"k\")\n","        plt.axis(\"off\")\n","        plt.show()"],"metadata":{"id":"Wo8FMOY0qe99"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Although the PINN clearly didn't manage to capture the spike at the end of the time period we're looking at, it was much more successful than the 'physics-less' neural network in predicting that there would be a peak and subsequent fall-off after the end of the training period.\n","\n","We trained the model for long enough for it to fit the data and had we ran it for longer, it's possible that the output would change and no longer be quite so correct. This may seem like we're cheating and getting the correct answer because we know what it is. In a sense it's true that knowing the correct prediction is very helpful in knowing when to stop training, but this is a general flaw of using neural networks for this type of predictive task. The question of when to stop training when working with neural networks is more of an art than an exact science; train too long and you will 'overfit' the data (i.e. fit the training data really well, but not generalise well beyond the training set), but train too little and you risk losing important information hidden in the training set.\n","\n","The problem would be much simpler to solve if we weren't constricted to data on just one COVID-19 epidemic. If we had data on thousands of different epidemics, we could abandon the PINN idea and completely change the architecture of the neural network, so that its job is to predict the cases during the second half of the time period based on knowledge of the first half of the period. In this case, overfitting would be much easier to monitor without ever checking the output of the model for the epidemic that we want to find out information about.\n","\n","Using more detailed models of the epidemic dynamics such as the SAIRD model (which differentiates between symptomatic and assymptomatic, recovered and dead) could help make more accurate, longer term predictions with our PINN - although we did adjust the SIR model by giving $\\beta$ the ability to vary with time, it is still quite a crude model of a rather complex system. With that said, there is a chance it could be improved by simply making the time dependence of $\\beta$ more informed by including information on current affairs such as changes in public health policy in the dataset (using newspaper headlines perhaps).\n","\n","As mentioned earlier, there are several academic papers and preprints out there on the topic of using PINNs to retrospectively predict COVID-19 spikes and dips - here are some links if you feel like having a read or trying to recreate their results:\n","\n","[Approaching epidemiological dynamics of COVID-19 with physics-informed neural networks](https://arxiv.org/abs/2302.08796)\n","\n","PINN Training using Biobjective Optimization:\n","The Trade-off between Data Loss and Residual Loss - [Github](https://git.uni-wuppertal.de/heldmann/covid-prediction-pinn) & [ArXiv](https://arxiv.org/abs/2302.01810)"],"metadata":{"id":"nqKYla23yT_u"}}]}